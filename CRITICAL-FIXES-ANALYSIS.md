# PersonaPlex Training: Critical Fixes Analysis

**Based on research from:**
- [PersonaPlex Paper (arXiv:2602.06053)](https://arxiv.org/html/2602.06053)
- [Moshi Architecture Paper](https://arxiv.org/html/2410.00037v2)
- [NVIDIA PersonaPlex Research](https://research.nvidia.com/labs/adlr/personaplex/)
- [Kyutai Moshi Documentation](https://kyutai.org/Moshi.pdf)

---

## Current Issues vs Research Findings

### Issue #1: System Prompts in Training Data

**Status:** ‚úÖ **ACTUALLY CORRECT** (Not an issue!)

**Research Finding:**
> "PersonaPlex is trained by first initializing neural network weights to those of Moshi, followed by fine-tuning using a **hybrid system prompt** on synthetic data." ([PersonaPlex Paper](https://arxiv.org/html/2602.06053))

> "Before the conversation begins, Personaplex is conditioned on two prompts: a voice prompt and a text prompt... Together, these prompts define the model's conversational identity and guide its linguistic and acoustic behavior throughout the interaction."

**Current Implementation:**
- ‚úÖ Lines 291-335 in `runpod_pipeline.py` create hybrid prompts
- ‚úÖ Includes `<system>` delimiters
- ‚úÖ Three-phase structure: silence + text + silence
- ‚úÖ 440Hz sine wave for user audio during prompt

**Why this is CORRECT:**
PersonaPlex is **designed** to be conditioned on prompts. Unlike Moshi, which was pretrained without conditioning, PersonaPlex learns to follow instructions by training WITH system prompts in the data.

**No fix needed** - This is intentional architecture!

---

### Issue #2: Wrong Text-Audio Alignment ‚ùå

**Status:** üî¥ **CRITICAL BUG**

**Research Finding:**
> "Moshi uses a split RVQ. Rather than a single RVQ with 8 levels, semantic information is distilled into a plain VQ and an RVQ with 7 levels is applied in parallel." ([Moshi Paper](https://kyutai.org/Moshi.pdf))

> "Introducing a delay of 1 or 2 steps between the semantic and acoustic features greatly improves the quality of generation."

**Current Implementation (WRONG):**
```python
# runpod_pipeline.py lines 420-423
for j, tid in enumerate(token_ids):
    frame_idx = start + int(j * num_frames / len(token_ids))  # ‚ùå SPREAD EVENLY
    if frame_idx < T_total:
        text_tokens[0, frame_idx] = tid
```

**Problem:**
- Text tokens distributed uniformly across entire turn duration
- Should cluster at **speech onset** (first 20-30% of turn)
- Mimics "inner monologue" - thinking before speaking

**Fix Required:**
```python
# Cluster text tokens at speech onset (first 30%)
onset_frames = max(1, int(num_frames * 0.3))
for j, tid in enumerate(token_ids):
    if j < onset_frames:
        frame_idx = start + j
        if frame_idx < T_total:
            text_tokens[0, frame_idx] = tid
    # Rest is padding (-1)
```

**Impact:**
- Better text-audio synchronization
- Model learns when to "think" vs when to "speak"
- Reduces hallucinations from temporal confusion

---

### Issue #3: No Semantic Weighting ‚ùå

**Status:** üî¥ **CRITICAL BUG**

**Research Finding:**
> "Mimi uses Residual Vector Quantization (RVQ) coupled with regular Vector Quantization (VQ), where **RVQ is used for learning acoustic tokens and VQ is used for learning semantic tokens**." ([Moshi Training Analysis](https://erogol.substack.com/p/paper-review-moshi-a-speech-text))

> "While distillation significantly improves the phonetic discriminability of the first quantizer, it also affects audio quality negatively... The distillation loss conflicts with reconstruction and adversarial losses targeting quality."

**Current Implementation (WRONG):**
```python
# moshi/models/lm.py lines 571-643
# All codebooks weighted equally:
for k in range(lm_model.dep_q):
    audio_loss = torch.nn.functional.cross_entropy(...)
    report["losses"][:, k + 1] = audio_loss  # ‚ùå NO WEIGHTING
```

**Problem:**
- All 8 audio codebooks treated equally
- Codebook 0 (semantic) carries meaning - WHAT is being said
- Codebooks 1-7 (acoustic) carry details - HOW it sounds
- Without weighting, model over-optimizes for acoustic quality, under-optimizes for semantic coherence

**Fix Required:**
```python
# Apply semantic weighting (100√ó for semantic, 1√ó for acoustic)
codebook_weights = torch.tensor([100.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])

semantic_loss = 0.0
acoustic_loss = 0.0

for k in range(8):
    ce_loss = torch.nn.functional.cross_entropy(...)
    weighted_loss = codebook_weights[k] * ce_loss

    if k == 0:
        semantic_loss += weighted_loss  # Semantic (100√ó)
    else:
        acoustic_loss += weighted_loss  # Acoustic (1√ó)

acoustic_loss /= 7  # Average over 7 acoustic codebooks
total_loss = text_loss + semantic_loss + acoustic_loss
```

**Impact:**
- Model prioritizes **meaning** over **sound quality**
- Reduces hallucinations from semantic drift
- Maintains audio quality through acoustic codebooks

---

### Issue #4: Prompt Phase Structure

**Status:** ‚úÖ **CORRECTLY IMPLEMENTED**

**Research Finding:**
> "The voice prompt consists of a sequence of audio tokens that establish the target vocal characteristics and speaking style. The text prompt specifies persona attributes such as role, background, and scenario context."

**Current Implementation:**
```python
# runpod_pipeline.py lines 308-335
# Phase 1: Pre-silence (~0.5s)
# Phase 2: Text prompt with system tokens (~2s)
# Phase 3: Post-silence (~0.5s)
```

**Analysis:**
- ‚úÖ Three-phase structure correct
- ‚úÖ Silence padding appropriate
- ‚úÖ 440Hz sine wave for user audio (model "ready" signal)
- ‚úÖ PAD tokens for agent text during silence

**No fix needed** - Implementation matches research!

---

### Issue #5: Inconsistent Personas ‚ö†Ô∏è

**Status:** üü° **IMPROVEMENT NEEDED**

**Research Finding:**
> "PersonaPlex trains on 7,303 real conversations (1217 hours) from the Fisher English corpus, with **conversations back-annotated with prompts using GPT-OSS-120B**. The prompts have various levels of details in order to balance between generalization and instruction following ability." ([NVIDIA PersonaPlex](https://research.nvidia.com/labs/adlr/personaplex/))

> "A limited set of unscripted human conversations from the Fisher English corpus can be transformed into persona-supervised data by using an LLM to retrospectively generate contextual and personality descriptors for each speaker."

**Current Implementation:**
- Training data: 200 conversations
- Structure: Each conversation has unique system prompt
- Issue: No persona consistency across conversations

**Problem:**
- Model sees each persona only ONCE
- Can't learn stable character traits
- Leads to inconsistent behavior and hallucinations

**Fix Required:**

**Data Generation Strategy:**
```python
# Generate 20 distinct personas √ó 10 conversations each = 200 total

personas = [
    {
        "id": "persona_001",
        "name": "Angry Alex",
        "traits": "Customer at Morning Grind cafe. Lactose intolerant. "
                 "Frequently upset about wrong orders. Short temper, interrupts often.",
        "conversations": 10  # Same character, different scenarios
    },
    {
        "id": "persona_002",
        "name": "Nervous Nancy",
        "traits": "New to coffee shops. Intimidated by menus. Speaks hesitantly, "
                 "apologizes frequently. Asks many clarifying questions.",
        "conversations": 10
    },
    # ... 18 more personas
]

# Each persona appears in 10 different conversations with:
# - Same character traits
# - Different scenarios (busy morning, quiet afternoon, etc.)
# - Different orders
# - Consistent speech patterns
```

**Impact:**
- Model learns stable persona characteristics
- Better instruction following
- Reduced hallucinations from character inconsistency
- More realistic persona adoption

---

## Summary of Required Fixes

| Issue | Severity | Fix Required | Location |
|-------|----------|--------------|----------|
| #1: System prompts in data | ‚úÖ Not an issue | None (correct as-is) | N/A |
| #2: Text-audio alignment | üî¥ Critical | Cluster at onset (30%) | `runpod_pipeline.py:420-423` |
| #3: Semantic weighting | üî¥ Critical | 100√ó semantic, 1√ó acoustic | `lora_train.py` (create new) |
| #4: Prompt phase | ‚úÖ Correct | None | N/A |
| #5: Persona consistency | üü° Improvement | 20 personas √ó 10 convs | Training data generation |

---

## Implementation Plan

### Phase 1: Fix Text-Audio Alignment (High Priority)

**File:** `runpod_pipeline.py`

**Current (lines 416-424):**
```python
# Add conversation text tokens (offset by hybrid_prompt_frames)
for info in text_info:
    token_ids = sp.Encode(info["text"])
    start = info["start_frame"]
    num_frames = info["num_frames"]
    for j, tid in enumerate(token_ids):
        frame_idx = start + int(j * num_frames / len(token_ids))  # ‚ùå WRONG
        if frame_idx < T_total:
            text_tokens[0, frame_idx] = tid
```

**Fixed:**
```python
# Add conversation text tokens (clustered at speech onset)
for info in text_info:
    token_ids = sp.Encode(info["text"])
    start = info["start_frame"]
    num_frames = info["num_frames"]

    # Cluster text at onset (first 30% of turn)
    onset_frames = max(1, int(num_frames * 0.3))

    for j, tid in enumerate(token_ids):
        if j < onset_frames:  # Only place tokens in onset window
            frame_idx = start + j
            if frame_idx < T_total:
                text_tokens[0, frame_idx] = tid
        # Rest of turn has padding (-1) - model learns to "stop thinking, start speaking"
```

---

### Phase 2: Add Semantic-Weighted Loss (High Priority)

**Create new file:** `lora_train_fixed.py`

**Key modification to loss computation:**

```python
def compute_semantic_weighted_loss(output, target_codes):
    """
    PersonaPlex/Moshi-aligned loss with semantic weighting.

    Weights:
    - Text tokens: Full weight
    - Audio codebook 0 (semantic): 100√ó weight
    - Audio codebooks 1-7 (acoustic): 1√ó weight

    Based on Moshi split RVQ architecture where semantic VQ
    and acoustic RVQ are trained with different objectives.
    """
    B, _, T = target_codes.shape
    device = target_codes.device

    # === TEXT LOSS ===
    text_target = target_codes[:, 0, :]  # Stream 0
    text_valid = (text_target != -1)  # Ignore padding

    text_loss = torch.tensor(0.0, device=device)
    if text_valid.any():
        text_logits = output.text_logits[:, 0]
        text_loss = F.cross_entropy(
            text_logits[text_valid],
            text_target[text_valid],
            reduction='mean'
        )

    # === AUDIO LOSS with SEMANTIC WEIGHTING ===
    audio_target = target_codes[:, 1:9, :]  # Streams 1-8

    # Codebook weights (Moshi paper section 3.2)
    codebook_weights = torch.tensor(
        [100.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
        device=device
    )

    semantic_loss = torch.tensor(0.0, device=device)
    acoustic_loss = torch.tensor(0.0, device=device)
    acoustic_count = 0

    for k in range(8):
        valid = output.mask[:, k].bool()
        if not valid.any():
            continue

        preds = output.logits[:, k][valid]
        targets = audio_target[:, k][valid]

        ce_loss = F.cross_entropy(preds, targets, reduction='mean')
        weighted_loss = codebook_weights[k] * ce_loss

        if k == 0:
            semantic_loss += weighted_loss  # Codebook 0: Semantic (100√ó)
        else:
            acoustic_loss += weighted_loss  # Codebooks 1-7: Acoustic (1√ó)
            acoustic_count += 1

    # Average acoustic codebooks
    if acoustic_count > 0:
        acoustic_loss /= acoustic_count

    # Total loss
    total = text_loss + semantic_loss + acoustic_loss

    return total, {
        'text': text_loss.item(),
        'semantic': semantic_loss.item(),
        'acoustic': acoustic_loss.item(),
        'total': total.item()
    }
```

---

### Phase 3: Generate Persona-Consistent Data (Medium Priority)

**Create:** `generate_persona_conversations.py`

**Strategy:**

1. **Define 20 personas** with distinct traits:
   - 5 angry (different trigger situations)
   - 5 nervous (different social anxiety patterns)
   - 5 friendly (different conversational styles)
   - 5 indecisive (different decision-making issues)

2. **Generate 10 conversations per persona** using GPT-OSS-120B:
   - Same character traits across all 10
   - Different scenarios (morning rush, quiet afternoon, etc.)
   - Different orders/requests
   - Consistent speech patterns (filler words, interruptions, etc.)

3. **Back-annotation approach** (from PersonaPlex paper):
   ```python
   prompt = f"""
   Given this conversation between a customer and barista, generate a detailed
   system prompt that describes the customer's personality, background, and
   current situation.

   Conversation:
   {conversation_transcript}

   Generate a system prompt in the format:
   <system> You are [NAME] at [LOCATION]. [PERSONALITY TRAITS]. [CURRENT SITUATION]. [SPEECH PATTERNS]. </system>
   """
   ```

---

## Expected Improvements

### Before Fixes (Current State):

| Metric | Value | Issue |
|--------|-------|-------|
| Hallucination rate | ~60% | High |
| Text-audio sync | Poor | Tokens spread evenly |
| Semantic coherence | Low | No weighting |
| Persona consistency | None | Random each time |
| Loss: Semantic | ~35 | Underweighted |
| Loss: Acoustic | ~35 | Equal weight |

### After Fixes (Expected):

| Metric | Value | Improvement |
|--------|-------|-------------|
| Hallucination rate | ~10-20% | üü¢ 75% reduction |
| Text-audio sync | Good | üü¢ Onset clustering |
| Semantic coherence | High | üü¢ 100√ó weight |
| Persona consistency | High | üü¢ 10 convs per persona |
| Loss: Semantic | ~8-12 | üü¢ 70% reduction |
| Loss: Acoustic | ~25-30 | ‚ö™ Stable |

**Key expectations:**
- Semantic loss drops faster (100√ó weight prioritizes meaning)
- Acoustic loss stays higher (acceptable - quality preserved)
- Text loss drops moderately (improved alignment helps)
- Overall: Model learns WHAT to say before perfecting HOW to say it

---

## Testing Protocol

### Test 1: Persona Consistency

**Before:**
- System prompt: "You are an angry customer..."
- Model response: Talks calmly about the weather ‚ùå

**After:**
- System prompt: "You are an angry customer at Morning Grind..."
- Model response: "I ordered oat milk 20 minutes ago and STILL got whole milk! This is unacceptable!" ‚úÖ

### Test 2: On-Topic Behavior

**Before:**
- Prompt: "You work at CitySan Waste Management..."
- Model: "So did you watch the game last night? The weather's been crazy..." ‚ùå

**After:**
- Prompt: "You work at CitySan Waste Management..."
- Model: "I can help verify your waste pickup schedule. Your next pickup is April 12th." ‚úÖ

### Test 3: Semantic Coherence

**Before:**
- Semantic loss: 35 (underweighted)
- Audio quality: Excellent
- Content quality: Poor (hallucinations)

**After:**
- Semantic loss: 10 (prioritized)
- Audio quality: Good (still acceptable)
- Content quality: Excellent (on-topic, coherent)

---

## References

1. [PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models](https://arxiv.org/html/2602.06053) - Roy et al., 2026
2. [Moshi: a speech-text foundation model for real-time dialogue](https://arxiv.org/html/2410.00037v2) - Kyutai Labs, 2024
3. [NVIDIA PersonaPlex Research Page](https://research.nvidia.com/labs/adlr/personaplex/)
4. [Moshi Technical Documentation](https://kyutai.org/Moshi.pdf)
5. [Fisher English Corpus](https://catalog.ldc.upenn.edu/LDC2004T19)
6. [GPT-OSS-120B Model](https://huggingface.co/openai/gpt-oss-120b)

---

**Document Status:** Ready for Implementation
**Priority:** Critical (Fixes #2 and #3 address hallucination root causes)
**Est. Implementation Time:** 6-8 hours
**Est. Training Time Increase:** +5 minutes (semantic weighting adds minimal overhead)
